import logging
import requests
import pandas as pd
import pickle
import os
import time
from geopy.geocoders import Nominatim
import feedparser
from datetime import datetime, timedelta
import pytz
import re
import streamlit as st
from google.cloud import translate_v2 as translate  # pip install google-cloud-translate==2.0.1
from google.oauth2 import service_account
import config


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)


# Set verbose if needed
# globals.set_debug(True)


# # # # # # # # # #
# êµ¬ê¸€ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° #
# # # # # # # # # #


def get_google_outage_news(keyword_):
    query = keyword_
    if and_keyword:
        query += ' ' + and_keyword[0]

    url = f"https://news.google.com/rss/search?q={query}+when:{search_hour}h"
    url += f'&hl=en-US&gl=US&ceid=US:en'
    url = url.replace(' ', '%20')

    title_list = []
    source_list = []
    pubtime_list = []
    link_list = []

    try:
        res = requests.get(url)  # , verify=False)
        logging.info('ì›ë³¸ ë§í¬: ' + url)

        if res.status_code == 200:
            datas = feedparser.parse(res.text).entries
            for data in datas:
                title = data.title
                logging.info('êµ¬ê¸€ë‰´ìŠ¤ì œëª©(ì›ë³¸): ' + title)

                minus_index = title.rindex(' - ')
                title = title[:minus_index].strip()

                # ê¸°ì‚¬ ì œëª©ì— ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë„˜ê¸´ë‹¤.
                if keyword_.lower() not in title.lower():
                    continue

                title_list.append(title)
                source_list.append(data.source.title)
                link_list.append(data.link)

                pubtime = datetime.strptime(data.published, "%a, %d %b %Y %H:%M:%S %Z")
                # GMT+9 (Asia/Seoul)ìœ¼ë¡œ ë³€ê²½
                gmt_plus_9 = pytz.FixedOffset(540)  # 9 hours * 60 minutes = 540 minutes
                pubtime = pubtime.replace(tzinfo=pytz.utc).astimezone(gmt_plus_9)

                pubtime_str = pubtime.strftime('%Y-%m-%d %H:%M:%S')
                pubtime_list.append(pubtime_str)

        else:
            logging.error("Google ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨! Error Code: " + str(res.status_code))
            logging.error(str(res))
            return None

    except Exception as e:
        logging.error(e)
        logging.error("Google ë‰´ìŠ¤ RSS í”¼ë“œ ì¡°íšŒ ì˜¤ë¥˜ ë°œìƒ!")
        return None

    # ê²°ê³¼ë¥¼ dict í˜•íƒœë¡œ ì €ì¥
    result = {'ì œëª©': title_list, 'ì–¸ë¡ ì‚¬': source_list, 'ë°œí–‰ì‹œê°„': pubtime_list, 'ë§í¬': link_list}

    df = pd.DataFrame(result)
    return df


def display_news_df(ndf, keyword_):
    # st.divider()
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    if ndf is None or len(ndf) == 0:
        st.write(f'âœ… ê²€ìƒ‰ëœ ë‰´ìŠ¤ ì—†ìŠµë‹ˆë‹¤. ({current_time})')
        return

    # st.write('ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼')

    disp_cnt = 0
    for i, row in ndf.iterrows():
        # ì´ë¯¸ ì¶œë ¥í–ˆë˜ ë‰´ìŠ¤ë¼ë©´ ìŠ¤í‚µí•œë‹¤.
        if row['ì œëª©'] in st.session_state.news_list:
            logging.info('ë‰´ìŠ¤ ìŠ¤í‚µ!!! - ' + row['ì œëª©'])
            continue

        # ì¶œë ¥í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•œë‹¤.
        st.session_state.news_list.append(row['ì œëª©'])
        disp_cnt += 1

        # title = row['ì œëª©'].replace(keyword_, f':yellow-background[{keyword_}]')
        # logging.info('keyword: ' + keyword_)
        # logging.info('before: ' + row['ì œëª©'])
        title = re.sub(keyword_, f':blue-background[{keyword_}]', row['ì œëª©'], flags=re.IGNORECASE)
        if and_keyword:
            title = re.sub(and_keyword[0], f':blue-background[{and_keyword[0]}]', title, flags=re.IGNORECASE)
        # logging.info('after : ' + title)

        # ì œëª© ë²ˆì—­
        korean_title = translate_eng_to_kor(row['ì œëª©'])

        with st.container(border=True):
            st.markdown(f'**{title}**')
            st.caption(f'{korean_title}')
            st.markdown(f'- {row["ì–¸ë¡ ì‚¬"]}, {row["ë°œí–‰ì‹œê°„"]} <a href="{row["ë§í¬"]}" target="_blank">ğŸ“</a>',
                        unsafe_allow_html=True)
        # st.write(' - ì–¸ë¡ ì‚¬: ' + row['ì–¸ë¡ ì‚¬'] + '  - ë°œí–‰ì‹œê°: ' + row['ë°œí–‰ì‹œê°„'])

    if disp_cnt > 0:
        st.write(f'âœ… ë‰´ìŠ¤ í‘œì‹œ ì™„ë£Œ ({current_time})')
    else:
        st.write(f'âœ… ì‹ ê·œ ë‰´ìŠ¤ ì—†ìŠµë‹ˆë‹¤. ({current_time})')


def fetch_news(keyword_, infinite_loop=False):
    with st.spinner('ë‰´ìŠ¤ ê²€ìƒ‰ì¤‘...'):
        news_df_ = get_google_outage_news(keyword_)
        # st.write(news_df_)
        display_news_df(news_df_, keyword_)

    while infinite_loop:
        time.sleep(st.session_state.search_interval_min * 60)
        with st.spinner('ë‰´ìŠ¤ ê²€ìƒ‰ì¤‘...'):
            news_df_ = get_google_outage_news(keyword_)
            # st.write(news_df_)
            display_news_df(news_df_, keyword_)


# # # # # # # # # # # # # # #
# ì˜ì–´ ë²ˆì—­
# # # # # # # # # # # # # # #


def translate_eng_to_kor(text):
    # ìºì‹œë¥¼ ë¨¼ì € ë’¤ì ¸ë³¸ë‹¤.
    cache_text = load_trans_cache(text)
    if cache_text:
        logging.info('trans cache hit! - ' + text + ' : ' + cache_text)
        return cache_text  # ìºì‹œí›!

    # ìºì‹œì— ì—†ìœ¼ë©´ êµ¬ê¸€ apië¡œ ë²ˆì—­ì„ í•œë‹¤.
    if not os.path.exists(config.KEY_PATH):
        return ''

    credential_trans = service_account.Credentials.from_service_account_file(config.KEY_PATH)
    translate_client = translate.Client(credentials=credential_trans)

    result = translate_client.translate(text, target_language='ko')
    # print(names)
    translated_text = result['translatedText'].replace('&amp;', '&')

    # ìºì‹œì— ì €ì¥í•œë‹¤.
    save_trans_cache(text, translated_text)

    return translated_text


def save_trans_cache(eng_text, kor_text):
    if len(st.session_state.trans_text_list) >= 100:  # 100ê°œ ì´í•˜ë¡œ ìœ ì§€í•œë‹¤.
        st.session_state.trans_text_list.pop(0)

    st.session_state.trans_text_list.append((eng_text, kor_text))  # ë²ˆì—­ íŠœí”Œì„ ë¦¬ìŠ¤íŠ¸ì— ì‚½ì…
    # ìºì‹œ íŒŒì¼ì— ì €ì¥
    with open(config.TRANS_CACHE_FILE, 'wb') as f_:
        pickle.dump(st.session_state.trans_text_list, f_)
        logging.info('ë²ˆì—­ ìºì‹œ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ')


def load_trans_cache(eng_text):
    for e_txt, k_txt in st.session_state.trans_text_list:
        if eng_text == e_txt:
            return k_txt  # ìºì‹œ í›
    return None


# # # # # # # # # # # # # # #
# ì‹œê°„ëŒ€ ë³€í™˜
# # # # # # # # # # # # # # #


def get_korean_time():
    # ì°¨íŠ¸ ì‹œê°„ëŒ€ ë³´ì •
    now = datetime.now() - timedelta(minutes=5)
    before_10unit_minute = int(now.minute / 10) * 10
    new_time = now.replace(minute=before_10unit_minute, second=0, microsecond=0)

    # 4ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ì´ì „ ì—¬ì„¯ê°œì˜ ì‹œê°ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    previous_times = [new_time]

    # 4ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ì´ì „ ì‹œê°„ë“¤ì„ ê³„ì‚°
    for i in range(1, 7):
        previous_time = new_time - timedelta(hours=4 * i)
        previous_times.append(previous_time)

    # ê²°ê³¼ ì¶œë ¥
    logging.info(str(previous_times))

    previous_times.reverse()
    return previous_times


# # # # # # # # # #
# ìœ„ê²½ë„ ë°›ì•„ì˜¤ê¸°
# # # # # # # # # #


def save_loc_cache(loc, lat, lon):
    st.session_state.geolocations_dict[loc] = {'lat': lat, 'lon': lon}
    # ìƒˆë¡œìš´ ìœ„ê²½ë„ ì •ë³´ë¥¼ ìºì‹œ íŒŒì¼ì— ì €ì¥
    with open(config.GEOLOC_CACHE_FILE, 'wb') as f_:
        pickle.dump(st.session_state.geolocations_dict, f_)
        logging.info('ìœ„ê²½ë„ ìºì‹œ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ')


def load_loc_cache(loc):
    return st.session_state.geolocations_dict.get(loc)


def get_geo_location(map_df_):
    geolocator = Nominatim(user_agent="jason")
    map_df_['lat'] = None
    map_df_['lon'] = None
    map_df_['color'] = '#ff000077'  # ë¹¨ê°•, ì‚´ì§ íˆ¬ëª….

    for i, row in map_df_.iterrows():
        # ì„¸ì…˜ ìºì‹œë¥¼ ë¨¼ì € ì‚´í´ë³¸ë‹¤.
        cache = load_loc_cache(row['Location'])
        if cache:
            logging.info('geo cache hit! - ' + row['Location'])
            map_df_.loc[i, 'lat'] = cache['lat']
            map_df_.loc[i, 'lon'] = cache['lon']
            continue

        # ì„¸ì…˜ ìºì‹œì— ì—†ìœ¼ë©´ ìœ„ê²½ë„ apië¥¼ ì¨ì„œ ë¶ˆëŸ¬ì˜¨ë‹¤.
        geo = geolocator.geocode(row['Location'])

        if geo:
            map_df_.loc[i, 'lat'] = geo.latitude
            map_df_.loc[i, 'lon'] = geo.longitude
            save_loc_cache(row['Location'], geo.latitude, geo.longitude)
        else:
            # retry
            geo = geolocator.geocode(row['Location'].split(',')[0])

            if geo:
                map_df_.loc[i, 'lat'] = geo.latitude
                map_df_.loc[i, 'lon'] = geo.longitude
                save_loc_cache(row['Location'], geo.latitude, geo.longitude)
            else:
                # retryê¹Œì§€ ì‹¤íŒ¨í•  ê²½ìš°.
                logging.error('Geo ERROR!!! :' + str(geo))

        time.sleep(0.2)
    return map_df_


def get_multiple(values_sr):
    max_report = values_sr.max()
    multiple_ = int(500000 / max_report)
    logging.info(f'{max_report=} {multiple_=}')
    return multiple_


# # # # # # # # # #
# ì›¹ í˜ì´ì§€ êµ¬ì„±
# # # # # # # # # #


st.set_page_config(layout="wide")
# st.title('ë‰´ìŠ¤ ê²€ìƒ‰ ë´‡')


# # # # # # # # # # # # # # #
# ì‚¬ì´ë“œë°”
# # # # # # # # # # # # # # #


# st.sidebar.header('Global Service News Tracker')

total_services_list = []
for area in st.session_state.companies_list_dict:
    total_services_list += st.session_state.companies_list_dict[area]

total_services_list = list(set(total_services_list))
total_services_list.sort(key=lambda x: x.lower())

if 'selected_service_name' in st.session_state and st.session_state.selected_service_name is not None \
        and st.session_state.selected_service_name in total_services_list:
    item_index = total_services_list.index(st.session_state.selected_service_name)
else:
    item_index = None

service_code_name = st.sidebar.selectbox(
    "ê²€ìƒ‰ì„ ì›í•˜ëŠ” ì„œë¹„ìŠ¤ëŠ”?",
    total_services_list,
    index=item_index,
    placeholder="ì„œë¹„ìŠ¤ ì´ë¦„ ì„ íƒ...",
)


search_hour = st.sidebar.number_input('ìµœê·¼ ëª‡ì‹œê°„ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í• ê¹Œìš”?', value=1, format='%d')

and_keyword = st.sidebar.multiselect("ë‰´ìŠ¤ ê²€ìƒ‰ ì¶”ê°€ í‚¤ì›Œë“œ", options=['outage', 'blackout', 'failure'], default=['outage'])

st.session_state.search_interval_min = st.sidebar.number_input('ìƒˆë¡œê³ ì¹¨ ì£¼ê¸°(ë¶„)',
                                                               value=st.session_state.search_interval_min,
                                                               format='%d')

st.sidebar.divider()
st.sidebar.write('â“ https://downdetector.com')


if not os.path.exists(config.KEY_PATH):
    uploaded_file = st.sidebar.file_uploader('API Key File', type=['json'], accept_multiple_files=False)
else:
    logging.info('API key íŒŒì¼ì€ ë¡œì»¬ ì €ì¥ëœ íŒŒì¼ ì‚¬ìš©í•©ë‹ˆë‹¤.')
    uploaded_file = None


# key json íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
if uploaded_file is not None:
    # íŒŒì¼ì„ ë¡œì»¬ì— ì €ì¥
    with open(config.KEY_PATH, "wb") as file:
        file.write(uploaded_file.getbuffer())

    st.toast(f"API key file has been saved successfully!")


# # # # # # # # # # # # # # #
# ë©”ì¸ í™”ë©´
# # # # # # # # # # # # # # #


# ì„œë¹„ìŠ¤ ì„ íƒì‹œ ì²˜ë¦¬
if service_code_name:
    # ë³¸ë¬¸ í™”ë©´ êµ¬ì„±
    title_placeholder = st.empty()
    col1, col2 = st.columns(2)

    # ë¹ˆ ê³µê°„ì„ ìƒì„±í•˜ì—¬ ë‚˜ì¤‘ì— ë‚´ìš©ì„ ì—…ë°ì´íŠ¸í•  ì¤€ë¹„
    col1_placeholder = col1.empty()
    col2_placeholder = col2.empty()

    with st.spinner('ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒì¤‘...'):
        status, report_list, _ = config.get_service_chart_mapdf(area=st.session_state.selected_area,
                                                                service_name=service_code_name)

        if status is None:
            with title_placeholder.container():
                st.subheader(f'**{service_code_name}**')
        else:
            # ìƒíƒœ
            color, color_code, icon = config.get_status_color(service_code_name, status)

            with title_placeholder.container():
                st.subheader(f'**{service_code_name}**  :{color}[{icon}]')

    # ì»¬ëŸ¼2 - ì°¨íŠ¸
    with col2_placeholder.container():
        if report_list is not None:
            st.write('ğŸ“ˆ Live Report Chart (Last 24 hours)')

            with st.container():
                chart_data = pd.DataFrame(report_list, columns=["Report Count"])
                st.line_chart(chart_data, color=color_code)
        else:
            st.write('')  # no report chart

    # ì»¬ëŸ¼1 - ë‰´ìŠ¤
    with col1_placeholder.container():
        st.session_state.news_list = []  # ë‰´ìŠ¤ ì„¸ì…˜ í´ë¦¬ì–´
        st.write('ğŸ“° News List')
        fetch_news(service_code_name)


# ì£¼ê¸°ì ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•œë‹¤.
# ì‚¬ì´ë“œë°”ì— íƒ€ì´ë¨¸ í‘œê¸°
st.sidebar.divider()

# íƒ€ì´ë¨¸ë¥¼ í‘œì‹œí•  ìœ„ì¹˜ ì˜ˆì•½
timer_placeholder = st.sidebar.empty()

# ì¹´ìš´íŠ¸ë‹¤ìš´ ì´ˆ ê³„ì‚°
if service_code_name:
    if st.session_state.search_interval_timer_cache <= 0:
        st.session_state.search_interval_timer_cache = st.session_state.search_interval_min * 60

    # íƒ€ì´ë¨¸ ì‹¤í–‰
    while st.session_state.search_interval_timer_cache >= 0:
        # íƒ€ì´ë¨¸ ê°±ì‹ 
        timer_placeholder.markdown(f"â³ ì¬ê²€ìƒ‰ê¹Œì§€ {st.session_state.search_interval_timer_cache}ì´ˆ")

        # 1ì´ˆ ëŒ€ê¸°
        time.sleep(1)

        # íƒ€ì´ë¨¸ ê°ì†Œ
        st.session_state.search_interval_timer_cache -= 1

    # íƒ€ì´ë¨¸ ì™„ë£Œ ë©”ì‹œì§€
    timer_placeholder.markdown("â° ì¹´ìš´íŠ¸ë‹¤ìš´ ì™„ë£Œ! ì„œë¹„ìŠ¤ ìƒíƒœ ì¬ê²€ìƒ‰!")

    logging.info('ì¬ê²€ìƒ‰!!!')
    config.init_status_df()  # ì„œë¹„ìŠ¤ ìƒíƒœ ì´ˆê¸°í™”
    st.rerun()
