import os
import logging
import pickle
import streamlit as st
import get_downdetector_web
import time
import pandas as pd


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)


# íŒŒì¼ëª…
DASHBOARD_PAGE = 'dashboard_dd.py'
NEWSBOT_PAGE = 'news_bot_dd.py'

GEOLOC_CACHE_FILE = 'geolocation_cache.pkl'
TRANS_CACHE_FILE = 'trans_cache.pkl'
KEY_PATH = 'key.json'

COMPANIES_LIST_FILE = 'companies_list_dd.pkl'


# ìƒ‰ìƒ ì½”ë“œ
GREEN = '#66FF66BB'
ORANGE = '#FFCC66BB'
RED = '#FF6666BB'


DEFAULT_COMPANIES_SET_DICT = {
    'US': {
        'Amazon',
        'Amazon Web Services',
        'AT&T',
        'Cloudflare',
        'Discord',
        'Disney+',
        'Facebook',

        'Gmail',
        'Google',
        'Google Calendar',
        'Google Cloud',
        'Google Drive',
        'Google Duo',
        'Google Maps',
        'Google Meet',
        'Google Play',
        'Google Public DNS',
        'Google Workspace',

        'iCloud',
        'Instagram',
        'Line',
        'Microsoft 365',
        'Minecraft',
        'Netflix',
        'OpenAI',
        'Paramount+',
        'Paypal',
        'Roblox',
        'Snapchat',
        'Spotify',
        'Starlink',
        'T-Mobile',
        'TikTok',
        'Twitch',
        'Verizon',
        'Whatsapp',
        'X (Twitter)',
        'Yahoo',
        'Yahoo Mail',
        'Youtube',
    },

    'JP': {
        'Akamai',
        'Amazon',
        'Amazon Web Services',
        'App Store',
        'Apple Store',
        'Cloudflare',
        'Dropbox',
        'Facebook',

        'Gmail',
        'Google',
        'Google Calendar',
        'Google Cloud',
        'Google Drive',
        'Google Duo',
        'Google Maps',
        'Google Meet',
        'Google Play',
        'Google Public DNS',
        'Google Workspace',

        'iCloud',
        'Instagram',
        'Line',
        'Microsoft 365',
        'Microsoft Azure',
        'Microsoft Teams',
        'Netflix',
        'NTT Docomo',
        'OpenAI',
        'SoftBank',
        'TikTok',
        'Whatsapp',
        'X (Twitter)',
        'Yahoo',
        'Yahoo Mail',
        'Youtube',
        'Zoom',
    }
}


# # # # # # # # # # # # # # #
# í”¼í´ íŒŒì¼ ë¡œë”© í•¨ìˆ˜
# # # # # # # # # # # # # # #


def pickle_load_cache_file(filename, default_type):
    if os.path.exists(filename):
        # ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¨ë‹¤.
        with open(filename, 'rb') as pickle_f:
            loaded_object = pickle.load(pickle_f)
            logging.info('í”¼í´ ìºì‹œ íŒŒì¼ ë¡œë”© ì™„ë£Œ : ' + filename)
            return loaded_object

    logging.info('í”¼í´ íŒŒì¼ ì—†ìŒ! : ' + filename)
    return default_type()


# # # # # # # # # # # # # # #
# ì„œë¹„ìŠ¤ì˜ í˜„ì¬ ìƒíƒœ ë°›ì•„ì˜¤ê¸°
# # # # # # # # # # # # # # #


def get_service_chart_df_by_url_list(area):
    logging.info(f'===== {area} ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ =====')

    if area.upper() == 'JP':
        postfix = 'jp'
    else:
        postfix = 'com'

    url_list = [f'https://downdetector.{postfix}/',
                f'https://downdetector.{postfix}/telecom/',
                f'https://downdetector.{postfix}/online-services/',
                f'https://downdetector.{postfix}/social-media/'
                ]

    df_list = []
    for url_item in url_list:
        df_ = get_downdetector_web.get_downdetector_df(url=url_item, area=area)
        if df_ is not None:
            df_list.append(df_)
        time.sleep(1)  # guard time

    if len(df_list) == 0:
        logging.info(f'===== {area} ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨!!! =====')
        return None

    total_df = (pd.concat(df_list, ignore_index=True)
                .drop_duplicates(subset=get_downdetector_web.NAME, keep='first'))

    logging.info(f'===== {area} ì „ì²´ í¬ë¡¤ë§ ë° df ë³€í™˜ ì™„ë£Œ =====')
    return total_df


def refresh_status_and_save_companies(area):
    # ìƒíƒœ ë°›ì•„ì˜¤ê¸°
    st.session_state.status_df_dict[area] = get_service_chart_df_by_url_list(area)

    if st.session_state.status_df_dict[area] is None:
        return

    # íšŒì‚¬ ëª©ë¡ íŒŒì¼ ì—…ë°ì´íŠ¸
    new_list = list(st.session_state.status_df_dict[area][get_downdetector_web.NAME])

    # ê¸°ì¡´ íšŒì‚¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    companies_list = pickle_load_cache_file(COMPANIES_LIST_FILE, dict)

    # ì‹ ê·œ ëª©ë¡ í•©ì¹˜ê¸°
    companies_list[area] = list(set(companies_list.get(area, []) + new_list))
    companies_list[area].sort(key=lambda x: x.lower())  # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì—†ì´ abc ìˆœìœ¼ë¡œ ì •ë ¬

    logging.info(f'{area} íšŒì‚¬ ëª©ë¡:\n' + str(companies_list[area]))
    logging.info(f'{area} Total companies count: ' + str(len(companies_list[area])))

    # í•©ì³ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with open(COMPANIES_LIST_FILE, 'wb') as f_:
        pickle.dump(companies_list, f_)
        logging.info(f'{area} íšŒì‚¬ ëª©ë¡ ì—…ë°ì´íŠ¸ & íŒŒì¼ ì €ì¥ ì™„ë£Œ')


def get_service_chart_mapdf(area, service_name=None, need_map=False):
    # ìµœì´ˆ ë¡œë”© ì‹œ ë˜ëŠ” service_name Noneì¼ ê²½ìš°
    if st.session_state.status_df_dict.get(area) is None or service_name is None:
        refresh_status_and_save_companies(area)

    # í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆì„ ê²½ìš° ë˜ëŠ” ë‹¨ìˆœ í¬ë¡¤ë§ ëª©ì ì˜ í˜¸ì¶œì¼ ê²½ìš°
    if st.session_state.status_df_dict.get(area) is None or service_name is None:
        return None, None, None

    for i, row in st.session_state.status_df_dict[area].iterrows():
        if row[get_downdetector_web.NAME].upper() == service_name.upper() \
                and row[get_downdetector_web.AREA].upper() == area.upper():  # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì´ë¦„/ì§€ì—­ ì¼ì¹˜ ì°¾ìŒ.
            # ì„œë¹„ìŠ¤ë¥¼ ì°¾ìœ¼ë©´ í´ë˜ìŠ¤, ë¦¬í¬íŠ¸ ë¦¬ìŠ¤íŠ¸, ì§€ë„ë¥¼ ë¦¬í„´í•¨.
            data_values = [int(x) for x in row[get_downdetector_web.VALUES].strip('[]').split(', ')]
            return row[get_downdetector_web.CLASS], data_values, None

    # ì„œë¹„ìŠ¤ë¥¼ ëª»ì°¾ì•˜ì„ ê²½ìš°
    return None, None, None


# í˜„ì¬ ì•ŒëŒì´ ëœ¬ ì„œë¹„ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_current_alarm_service_list(area):
    if st.session_state.status_df_dict.get(area) is None:
        get_service_chart_mapdf(area)  # í˜„ì¬ ê°’ì´ ì—†ì„ ê²½ìš° ê°•ì œ í¬ë¡¤ë§ 1íšŒ ìˆ˜í–‰.

    alarm_list = []

    for i, row in st.session_state.status_df_dict[area].iterrows():
        if row[get_downdetector_web.CLASS] == get_downdetector_web.DANGER \
                and row[get_downdetector_web.AREA].upper() == area.upper():  # í•´ë‹¹ ì§€ì—­ì˜ Red ì•ŒëŒ
            alarm_list.append(row[get_downdetector_web.NAME])

    return alarm_list


def init_status_df():
    logging.info('status_df_dict ì´ˆê¸°í™”!')
    st.session_state.status_df_dict = dict()


def get_status_color(name, status):
    if status is None or status == get_downdetector_web.SUCCESS:
        color = 'green'
        color_code = GREEN
        icon = 'â˜»'
    elif status == get_downdetector_web.WARNING:
        color = 'orange'
        color_code = ORANGE
        icon = 'â˜ï¸'
        st.toast(f'**{name}** ì„œë¹„ìŠ¤ ë¬¸ì œ ë°œìƒ!', icon="ğŸ””")
    else:  # get_downdetector_web.DANGER:
        color = 'red'
        color_code = RED
        icon = 'â˜ ï¸'
        st.toast(f'**{name}** ì„œë¹„ìŠ¤ ì¤‘ëŒ€ ë¬¸ì œ ë°œìƒ!', icon="ğŸš¨")

    return color, color_code, icon

