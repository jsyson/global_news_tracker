import os
import logging
import pickle
import streamlit as st
import time
import pandas as pd
from datetime import datetime
import pytz


# í•œêµ­ ì‹œê°„ëŒ€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ìƒì„±
class KSTFormatter(logging.Formatter):
    def formatTime(self, record, datefmt=None):
        kst = pytz.timezone('Asia/Seoul')
        # record.createdë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ , ë³€í™˜ëœ ì‹œê°„ì„ ìƒì„±
        created_time = datetime.fromtimestamp(record.created, kst)
        if datefmt:
            return created_time.strftime(datefmt)
        else:
            return created_time.strftime('%Y-%m-%d %H:%M:%S')


# ë¡œê·¸ í¬ë§· ì„¤ì • (í•œêµ­ ì‹œê°„ëŒ€ í¬í•¨)
log_format = '%(asctime)s - %(levelname)s : %(message)s'

# ë¡œê¹… ì„¤ì •
# ë¡œê·¸ í•¸ë“¤ëŸ¬ ì„¤ì •
handler = logging.StreamHandler()
handler.setFormatter(KSTFormatter(log_format))

# ê¸°ë³¸ ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO, handlers=[handler])


# # # # # # # # # # # # # # # # # # # #


import get_downdetector_web


# íŒŒì¼ëª… ë“± ê°ì¢… ì„¤ì •
AREA_LIST = ['US', 'JP']

DASHBOARD_US_PAGE = 'pages/dashboard_us.py'
DASHBOARD_JP_PAGE = 'pages/dashboard_jp.py'
NEWSBOT_PAGE = 'pages/news_bot_dd.py'

GEOLOC_CACHE_FILE = 'geolocation_cache.pkl'
TRANS_CACHE_FILE = 'trans_cache.pkl'
KEY_PATH = 'key.json'

COMPANIES_LIST_FILE = 'companies_list_dd.pkl'


# ìƒ‰ìƒ ì½”ë“œ
GREEN = '#66FF66BB'
ORANGE = '#FFCC66BB'
RED = '#FF6666BB'


DEFAULT_COMPANIES_SET_DICT = {
    'US': {
        'Amazon',
        'Amazon Web Services',
        'AT&T',
        'Cloudflare',
        'Discord',
        'Disney+',
        'Facebook',

        'Gmail',
        'Google',
        # 'Google Calendar',
        'Google Cloud',
        'Google Drive',
        # 'Google Duo',
        'Google Maps',
        'Google Meet',
        'Google Play',
        # 'Google Public DNS',
        # 'Google Workspace',

        'iCloud',
        'Instagram',
        # 'Line',
        'Microsoft 365',
        # 'Minecraft',
        'Microsoft Azure',
        'Microsoft Teams',
        'Netflix',
        'OpenAI',
        # 'Paramount+',
        'Paypal',
        # 'Roblox',
        # 'Snapchat',
        # 'Spotify',
        'Starlink',
        'T-Mobile',
        'TikTok',
        # 'Twitch',
        'Verizon',
        # 'Whatsapp',
        'X (Twitter)',
        'Yahoo',
        'Yahoo Mail',
        'Youtube',
        # 'Zoom',
    },

    'JP': {
        'Akamai',
        'Amazon',
        'Amazon Web Services',
        'App Store',
        'Apple Store',
        'Cloudflare',
        'Dropbox',
        'Facebook',

        'Gmail',
        'Google',
        # 'Google Calendar',
        'Google Cloud',
        'Google Drive',
        # 'Google Duo',
        'Google Maps',
        'Google Meet',
        'Google Play',
        # 'Google Public DNS',
        # 'Google Workspace',

        'iCloud',
        'Instagram',
        'Line',
        'Microsoft 365',
        'Microsoft Azure',
        'Microsoft Teams',
        'Netflix',
        'NTT Docomo',
        'OpenAI',
        'SoftBank',
        'TikTok',
        # 'Whatsapp',
        'X (Twitter)',
        'Yahoo',
        'Yahoo Mail',
        'Youtube',
        'Zoom',
    }
}


# # # # # # # # # # # # # # #
# ì „ì²´ ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™”
# # # # # # # # # # # # # # #


def init_session_state():
    # ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™”(ê³µìš©)
    if 'selected_service_name' not in st.session_state:
        st.session_state.selected_service_name = None

    if 'selected_area' not in st.session_state:
        st.session_state.selected_area = None

    if "companies_list_dict" not in st.session_state:
        st.session_state.companies_list_dict = pickle_load_cache_file(COMPANIES_LIST_FILE, dict)

    # ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™”(ëŒ€ì‹œë³´ë“œ)
    if 'dashboard_auto_tab_timer' not in st.session_state:
        st.session_state.dashboard_auto_tab_timer = 10

    if 'auto_tab_timer_cache' not in st.session_state:
        st.session_state.auto_tab_timer_cache = -1

    if 'status_df_dict' not in st.session_state:
        st.session_state.status_df_dict = dict()

    if 'game_df_dict' not in st.session_state:
        st.session_state.game_df_dict = dict()

    if 'target_service_set_dict' not in st.session_state:
        st.session_state.target_service_set_dict = DEFAULT_COMPANIES_SET_DICT

    if 'status_cache' not in st.session_state:
        st.session_state.status_cache = dict()

    if 'dashboard_refresh_timer' not in st.session_state:
        st.session_state.dashboard_refresh_timer = 5

    if 'refresh_timer_cache' not in st.session_state:
        st.session_state.refresh_timer_cache = -1

    if 'num_dashboard_columns' not in st.session_state:
        st.session_state.num_dashboard_columns = 8

    if 'display_chart' not in st.session_state:
        st.session_state.display_chart = True

    # ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™”(ë‰´ìŠ¤)
    if "geolocations_dict" not in st.session_state:
        st.session_state.geolocations_dict = pickle_load_cache_file(GEOLOC_CACHE_FILE, dict)

    if 'trans_text_list' not in st.session_state:
        st.session_state.trans_text_list = pickle_load_cache_file(TRANS_CACHE_FILE, list)

    if "news_list" not in st.session_state:
        st.session_state.news_list = []

    if 'search_interval_timer_cache' not in st.session_state:
        st.session_state.search_interval_timer_cache = -1

    if 'search_interval_min' not in st.session_state:
        st.session_state.search_interval_min = 5


# # # # # # # # # # # # # # #
# í”¼í´ íŒŒì¼ ë¡œë”© í•¨ìˆ˜
# # # # # # # # # # # # # # #


def pickle_load_cache_file(filename, default_type):
    if os.path.exists(filename):
        # ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¨ë‹¤.
        with open(filename, 'rb') as pickle_f:
            loaded_object = pickle.load(pickle_f)
            logging.info('í”¼í´ ìºì‹œ íŒŒì¼ ë¡œë”© ì™„ë£Œ : ' + filename)
            return loaded_object

    logging.info('í”¼í´ íŒŒì¼ ì—†ìŒ! : ' + filename)
    return default_type()


# # # # # # # # # # # # # # #
# ì„œë¹„ìŠ¤ì˜ í˜„ì¬ ìƒíƒœ ë°›ì•„ì˜¤ê¸°
# # # # # # # # # # # # # # #


def get_service_chart_df_by_url_list(area):
    if area is None:
        logging.info(f'{area=} í¬ë¡¤ë§ ë¯¸ì‹¤í–‰!')
        return None

    logging.info(f'===== {area} ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ =====')

    if area.upper() == 'JP':
        postfix = 'jp'
    else:
        postfix = 'com'

    categories_list = [
        get_downdetector_web.TELECOM,
        get_downdetector_web.ONLINE_SERVICE,
        get_downdetector_web.SOCIAL_MEDIA,
        get_downdetector_web.FINANCE,
        get_downdetector_web.GAMING,
    ]

    # url_list = [  # f'https://downdetector.{postfix}/',
    #             f'https://downdetector.{postfix}/telecom/',
    #             f'https://downdetector.{postfix}/online-services/',
    #             f'https://downdetector.{postfix}/social-media/',
    #             f'https://downdetector.{postfix}/finance/',
    #             f'https://downdetector.{postfix}/gaming/',
    #             ]

    df_list = []
    for category_item in categories_list:
        url_item = f'https://downdetector.{postfix}/{category_item}/'
        df_ = get_downdetector_web.get_downdetector_df(url=url_item, area=area)
        if df_ is not None:
            df_[get_downdetector_web.CATEGORY] = category_item  # ì¢…ë¥˜ êµ¬ë¶„ì„ ì²¨ë¶€í•´ì¤€ë‹¤.
            df_list.append(df_)
        time.sleep(1)  # guard time

    if len(df_list) == 0:
        logging.error(f'===== {area} ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨!!! =====')
        return None

    total_df = (pd.concat(df_list, ignore_index=True)
                .drop_duplicates(subset=get_downdetector_web.NAME, keep='first'))

    logging.info(f'===== {area} ì „ì²´ í¬ë¡¤ë§ ë° df ë³€í™˜ ì™„ë£Œ =====')
    return total_df


def refresh_status_and_save_companies(area):
    # ì„¸ì…˜ìƒíƒœ ë°©ì–´ ì½”ë“œ
    init_session_state()

    # ìƒíƒœ ë°›ì•„ì˜¤ê¸°
    st.session_state.status_df_dict[area] = get_service_chart_df_by_url_list(area)

    if st.session_state.status_df_dict[area] is None or len(st.session_state.status_df_dict[area]) == 0:
        logging.error(f'{area} status_df ê°±ì‹  ì‹¤íŒ¨!')
        return

    logging.info(f'{area} status_df ê°±ì‹  í›„ ê¸¸ì´: {len(st.session_state.status_df_dict[area])}')

    # íšŒì‚¬ ëª©ë¡ íŒŒì¼ ì—…ë°ì´íŠ¸
    new_list = list(st.session_state.status_df_dict[area][get_downdetector_web.NAME])

    # ê¸°ì¡´ íšŒì‚¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    # companies_list = pickle_load_cache_file(COMPANIES_LIST_FILE, dict)

    # ì‹ ê·œ ëª©ë¡ í•©ì¹˜ê¸°
    st.session_state.companies_list_dict[area] = list(set(st.session_state.companies_list_dict.get(area, [])
                                                          + new_list))
    st.session_state.companies_list_dict[area].sort(key=lambda x: x.lower())  # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì—†ì´ abc ìˆœìœ¼ë¡œ ì •ë ¬

    # logging.info(f'{area} íšŒì‚¬ ëª©ë¡:\n{st.session_state.companies_list_dict[area][:5]} ...')
    logging.info(f'{area} Total services count: {len(st.session_state.companies_list_dict[area])}')

    # í•©ì³ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with open(COMPANIES_LIST_FILE, 'wb') as f_:
        pickle.dump(st.session_state.companies_list_dict, f_)
        logging.info(f'{area} íšŒì‚¬ ëª©ë¡ ì—…ë°ì´íŠ¸ & íŒŒì¼ ì €ì¥ ì™„ë£Œ')


def get_service_chart_mapdf(area, service_name=None, need_map=False):
    # ì„¸ì…˜ìƒíƒœ ë°©ì–´ ì½”ë“œ
    init_session_state()

    # ìµœì´ˆ ë¡œë”© ì‹œ ë˜ëŠ” service_name Noneì¼ ê²½ìš°
    if st.session_state.status_df_dict.get(area) is None or service_name is None:
        logging.info(f'ìµœì´ˆ ë¡œë”©ìœ¼ë¡œ ì˜ˆìƒ - ì„œë¹„ìŠ¤ ìƒíƒœ í¬ë¡¤ë§ ì‹œì‘ {area=} {service_name=}')
        refresh_status_and_save_companies(area)

    # ë‹¨ìˆœ í¬ë¡¤ë§ ëª©ì ì˜ í˜¸ì¶œì¼ ê²½ìš°
    if service_name is None:
        logging.info(f'í¬ë¡¤ë§ ì¢…ë£Œ - {area=}')
        return None, None, None

    # í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆì„ ê²½ìš°
    if st.session_state.status_df_dict.get(area) is None:
        logging.error(f'ì„œë¹„ìŠ¤ ìƒíƒœ í¬ë¡¤ë§ ì‹¤íŒ¨!!! {area=} {service_name=}')
        return None, None, None

    for i, row in st.session_state.status_df_dict[area].iterrows():
        if row[get_downdetector_web.NAME].upper() == service_name.upper() \
                and row[get_downdetector_web.AREA].upper() == area.upper():  # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì´ë¦„/ì§€ì—­ ì¼ì¹˜ ì°¾ìŒ.
            # ì„œë¹„ìŠ¤ë¥¼ ì°¾ìœ¼ë©´ í´ë˜ìŠ¤, ë¦¬í¬íŠ¸ ë¦¬ìŠ¤íŠ¸, ì§€ë„ë¥¼ ë¦¬í„´í•¨.
            if row[get_downdetector_web.VALUES] is None:
                logging.info(f'{area} {service_name} ì˜ data_values ì—†ìŒ!')
                data_values = None
            else:
                data_values = [int(x) for x in row[get_downdetector_web.VALUES].strip('[]').split(', ')]
            return row[get_downdetector_web.CLASS], data_values, None

    # ì„œë¹„ìŠ¤ë¥¼ ëª»ì°¾ì•˜ì„ ê²½ìš°
    return None, None, None


# í˜„ì¬ ì•ŒëŒì´ ëœ¬ ì„œë¹„ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_current_alarm_service_list(area):
    # ì„¸ì…˜ìƒíƒœ ë°©ì–´ ì½”ë“œ
    init_session_state()

    if st.session_state.status_df_dict.get(area) is None:
        logging.info('í˜„ì¬ ì•ŒëŒ ìƒíƒœ ì—†ì–´ì„œ í¬ë¡¤ë§ ì‹œì‘')
        get_service_chart_mapdf(area)  # í˜„ì¬ ê°’ì´ ì—†ì„ ê²½ìš° ê°•ì œ í¬ë¡¤ë§ 1íšŒ ìˆ˜í–‰.

    if st.session_state.status_df_dict.get(area) is None:
        # í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆì„ ê²½ìš°.
        logging.error(f'í¬ë¡¤ë§ ì‹¤íŒ¨í•˜ì—¬ í˜„ì¬ ì•ŒëŒ ìƒíƒœ í™•ì¸ ë¶ˆê°€!!!')
        return []

    alarm_list = []
    for i, row in st.session_state.status_df_dict[area].iterrows():
        # í•´ë‹¹ ì§€ì—­ì˜ Red ì•ŒëŒì´ë©´ì„œ ê²Œì„/ê¸ˆìœµ ì•ŒëŒì´ ì•„ë‹Œ ê²ƒ.
        if row[get_downdetector_web.CLASS] == get_downdetector_web.DANGER \
                and row[get_downdetector_web.AREA].upper() == area.upper() \
                and row[get_downdetector_web.CATEGORY] != get_downdetector_web.GAMING \
                and row[get_downdetector_web.CATEGORY] != get_downdetector_web.FINANCE:
            alarm_list.append(row[get_downdetector_web.NAME])

    logging.info(f'{area}ì˜ Red ì•ŒëŒ ì„œë¹„ìŠ¤ ëª©ë¡: {alarm_list}')

    # gameë“¤ì€ ì œì™¸.

    return alarm_list


def init_status_df():
    logging.info('status_df_dict ì´ˆê¸°í™”!')
    st.session_state.status_df_dict = dict()
    get_downdetector_web.get_downdetector_df.clear()


def get_status_color(name, status):
    if status is None or status == get_downdetector_web.SUCCESS:
        color = 'green'
        color_code = GREEN
        icon = 'â˜»'
    elif status == get_downdetector_web.WARNING:
        color = 'orange'
        color_code = ORANGE
        icon = 'â˜ï¸'
        # st.toast(f'**{name}** ì„œë¹„ìŠ¤ ë¬¸ì œ ë°œìƒ!', icon="ğŸ””")
    else:  # get_downdetector_web.DANGER:
        color = 'red'
        color_code = RED
        icon = 'â˜ ï¸'
        # st.toast(f'**{name}** ì„œë¹„ìŠ¤ ë¬¸ì œ ë°œìƒ!', icon="ğŸš¨")

    return color, color_code, icon

